This tutorial is for anaomaly detection in CPU usage data using statistical modeling. To ne more specidfic
we will be using a z score based technique. Model gets built with oultliers in data. The detected outliers
are removed and the model is built again, but htis time without outliers in the data.
 

Environment
===========
Path etc shown here corresposnds to my environment. Please Change them  as needed  for your 
environment

Build
=====
Follow instructions in spark_dependency.txt

Python dependency
=================
The shell script commands for data generation run python scripts for data generation. Before you run 
the data generation commands do the following
1. checkout project avenir
2. copy the avenir/python/lib directory to ../lib with respect to your location of cpu_usage.py file


Create base normal data
=======================
./and_spark.sh crInput <num_of_days> <reading_intervaL> <num_servers> true <output_file>

where
num_of_days = number of days e.g 10
reading_intervaL = reading interval in sec e.g. 300
num_servers = number of servers e.g. 4
output_file = output file, we will use cusage.txt from now on

- insert outliers
./and_spark.sh insOutliers <normal_data_file> <with_outlier_data_file> 

where
normal_data_file = normal data file (cusage.txt)
with_outlier_data_file = data file with outliers (cusage.txt)

-copy
./and_spark.sh cpModData <with_outlier_data_file> 

where
with_outlier_data_file = data file with outliers (cusage.txt)

Run Spark job for stats
=======================
./and_spark.sh numStat

Copy and consolidate stats file
===============================
./and_spark.sh crStatsFile

Run Spark job to detect outliers
================================
- set 
score.threshold = 2.0
output.outliers = true
rem.outliers = true

- run
./and_spark.sh olPred

Copy and consolidate clean file
===============================
./and_spark.sh crCleanFile

Create and copy test data
=========================
- create 
./and_spark.sh crInput <num_of_days> <reading_intervaL> <num_servers> true <stats_file> <output_file
where
stas_file = stats file path, which gets used to get all the server IDs

- insert outliers
./and_spark.sh insOutliers <normal_data_file> <with_outlier_data_file> 

where
normal_data_file = normal data file (c.txt)
with_outlier_data_file = data file with outliers (cusage.txt)

- copy
./and_spark.sh cpTestData <with_outlier_data_file> 

where
with_outlier_data_file = data file with outliers (cusage.txt)


Run Spark job for stats again with clean data
=============================================
./and_spark.sh numStat

Copy and consolidate stats file
===============================
./and_spark.sh crStatsFile


Run Spark job to detect outliers
================================
- set
score.threshold = 3.3
output.outliers = false
rem.outliers = false

- run
./and_spark.sh olPred

Configuration
=============
Configuration is in and.conf. Make changes as necessary



